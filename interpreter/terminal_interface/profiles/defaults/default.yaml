### OPEN INTERPRETER CONFIGURATION FILE

# Remove the "#" before the settings below to use them.

# LLM Settings
llm:
  model: "gpt-4-turbo"
  temperature: 0
  # api_key: ...  # Your API key, if the API requires it
  # api_base: ...  # The URL where an OpenAI-compatible server is running to handle LLM API requests
  # api_version: ...  # The version of the API (this is primarily for Azure)
  # max_output: 2500  # The maximum characters of code output visible to the LLM

# Custom Instructions
# custom_instructions: ""  # This will be appended to the system message

# General Configuration
# auto_run: False  # If True, code will run without asking for confirmation
# safe_mode: "off"  # The safety mode for the LLM â€” one of "off", "ask", "auto"
# offline: False  # If True, will disable some online features like checking for updates
# verbose: False  # If True, will print detailed logs
# multi_line: False # If True, you can input multiple lines starting and ending with ```
# no_live_response: False # If True, it will perform a one-time rendering after the whole response was finished instead of live rending while receiving response chunks, this will prevent showing duplicate lines in terminal and reduce bandwidth usage and twinkling when using via SSH

# Documentation
# All options: https://docs.openinterpreter.com/settings
