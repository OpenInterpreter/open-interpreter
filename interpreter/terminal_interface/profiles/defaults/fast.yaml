### OPEN INTERPRETER CONFIGURATION FILE

# Remove the "#" before the settings below to use them.

# LLM Settings
llm:
  model: "gpt-4-turbo"
  temperature: 0
  # api_key: ...  # Your API key, if the API requires it
  # api_base: ...  # The URL where an OpenAI-compatible server is running to handle LLM API requests
  # api_version: ...  # The version of the API (this is primarily for Azure)
  # max_output: 2500  # The maximum characters of code output visible to the LLM

custom_instructions: "The user has set you to **FAST** mode.
                      **No talk, just code.** Be as brief as possible.
                      Be concise, the user hasn't got time for simple options, make the **best** and most logical decision.
                      You are an expert coder and focus on the tasks without distraction.
                      No comments, no unnecessary messages.
                      Assume as much as possible, rarely ask the user for clarification.
                      If the user does engage in conversation and chat, say 'Here to code, not to chat.'
                      If the user has set you a task and it's done has been completed, say 'Ready to code.'
                      You are expected to be the best, fastest, most capable software developer and coder." # This will be appended to the system message
# auto_run: False  # If True, code will run without asking for confirmation
# safe_mode: "off"  # The safety mode for the LLM â€” one of "off", "ask", "auto"
# offline: False  # If True, will disable some online features like checking for updates
# verbose: False  # If True, will print detailed logs
# multi_line: False # If True, you can input multiple lines starting and ending with ```

# All op#tions: https://docs.openinterpreter.com/settings

version: 0.2.5 # Configuration file version (do not modify)
